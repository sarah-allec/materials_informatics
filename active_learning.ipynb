{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01bdf7bf",
   "metadata": {},
   "source": [
    "# Active (sequential) learning for experimental band gap prediction\n",
    "\n",
    "## Overview\n",
    "### Context\n",
    "The band gap of a material dictates many of its electrical properties and defines whether or not it is an insulator, a semiconductor, or a conductor. A model that can predict the experimental band gap of a given material, as well as guide/screen new materials is useful in a variety of electronic applications.\n",
    "### Problem formulation\n",
    "Develop a machine learning (ML) model that can predict the experimental band gap of a material given only its composition. Utilize active learning to guide the exploration of design space to optimize the band gap for a certain application. Here, we will seek to maximize the band gap (e.g., identify insulators), but one could use the same approach to minimize or optimize to a certain energy range.\n",
    "## Approach\n",
    "### 1. Data set importing.\n",
    "- Will use the `matbench_expt_gap` dataset\n",
    "    - For the sake of time, I filtered out all materials with a band gap of zero. Ideally, I would develop a classification model to identify metals as an initial pre-screening step before performing regression. Then for prediction on a new material, metals could automatically be assigned a band gap of zero and all others could use the trained regression model.\n",
    "- For now, will only use compositional features\n",
    "- See `eda.ipynb` for details regarding data cleaning and exploratory data analysis\n",
    "     \n",
    "### 2. As the output (i.e., target variable) is continuous, we will use regression.\n",
    "- I chose random forest because it is relatively robust, especially for smaller data sets (relative to neural networks)\n",
    "- Hyperparameter tuning and feature selection are done with `sklearn`\n",
    "- Final model training and prediction is done with `lolopy` because its random forest regressor has an option to return the standard deviation of the predictions from all trees (needed for two of the acquisition functions)\n",
    "     \n",
    "### 3. For active learning, we will test several acquisition functions for each step of training data additions.\n",
    "- MEI (Maximum Expected Improvement), MLI (Maximum Likelihood of Improvement) and MU (Maximum Uncertainty)\n",
    "    - Uncertainty estimates for random forest can be found [here](https://jmlr.org/papers/volume15/wager14a/wager14a.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbdcb10",
   "metadata": {},
   "source": [
    "# Import modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3f66002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General python \n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import helpers.plotting as my_plt\n",
    "\n",
    "# Machine learning training & prediction\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor as skl_RandomForestRegressor\n",
    "from lolopy.learners import RandomForestRegressor\n",
    "\n",
    "# Model persistence\n",
    "from joblib import dump, load\n",
    "\n",
    "# To ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99bf1037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning and feature selection\n",
    "def tune(X_df, y_ser, in_train = 'all', outf = 'grid_search_trained_model'):\n",
    "    \"\"\"\n",
    "    Performs an initial round of hyperparameter tuning with all features, then feature selection,\n",
    "    and finally a final round of hyperparameter tuning with top features from feature selection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_df : pandas dataframe \n",
    "            Input data\n",
    "    y_ser : pandas series\n",
    "            Target data\n",
    "    in_train: 'all' or list-like\n",
    "            Rows to include in training set\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_params : dict\n",
    "            Tuned hyperparameters\n",
    "    best_estimator : sklearn estimator\n",
    "            Tuned model\n",
    "    top_feat : list-like\n",
    "            Top features from feature selection\n",
    "    outf : str\n",
    "            Prefix for joblib file containing tuned model\n",
    "    \"\"\"\n",
    "    X = np.array(X_df)\n",
    "    y = np.array(y_ser)\n",
    "    if in_train == 'all':\n",
    "        in_train = list(range(len(y)))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "    else:\n",
    "        X_train = X[in_train]\n",
    "        y_train = y[in_train]\n",
    "        X_test = X[~in_train]\n",
    "        y_test = y[~in_train]\n",
    "    \n",
    "    # Hyperparameters to tune - here I only tune the number of trees and the maximum\n",
    "    # depth because these are the two primary common hyperparameters between\n",
    "    # sklearn's and lolopy's random forest regressors\n",
    "    dense_grid = {'n_estimators': [15, 20, 50, 100, 200],\n",
    "                  'max_depth': [5,10,20,50,100,1073741824]\n",
    "    }\n",
    "    # Hyperparameter tuning with sklearn\n",
    "    model = skl_RandomForestRegressor()\n",
    "    grid = GridSearchCV(estimator = model, param_grid = dense_grid, cv = 3, verbose=2,  n_jobs = -1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    best_estimator = grid.best_estimator_\n",
    "\n",
    "    # Feature selection\n",
    "    result = permutation_importance(\n",
    "        best_estimator, X_test, y_test, scoring = \"r2\", n_repeats=10, random_state=42\n",
    "    )\n",
    "    forest_importances = pd.Series(result.importances_mean, index=X_df.columns)\n",
    "    std = result.importances_std\n",
    "    sorted_ind = forest_importances.argsort()[::-1]\n",
    "    sorted_importances = forest_importances.sort_values(ascending=False)\n",
    "    if in_train == 'all':\n",
    "        top_feat = sorted_importances[ sorted_importances > 0.0 ].index\n",
    "    else:\n",
    "        top_feat = sorted_importances[ :10 ].index\n",
    "    X = np.array(df[top_feat])\n",
    "\n",
    "    # Final round of hyperparameter tuning\n",
    "    tmp = grid.fit(X_train, y_train)\n",
    "    best_params = grid.best_params_\n",
    "    best_estimator = grid.best_estimator_\n",
    "    # Save tuned model\n",
    "    dump(best_estimator, '{}.joblib'.format(outf))\n",
    "    return best_params, best_estimator, top_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3429e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Active learning: function to update training set\n",
    "def update(X, y_pred, y_std, train_inds, acq = 'mei', n = 5):\n",
    "    all_inds = set(range(len(X))) # indices for entire dataset\n",
    "    search_inds = list(all_inds.difference(train_inds)) # test set indices\n",
    "    acq = acq.lower()\n",
    "    if acq == 'mei':\n",
    "        return train_inds + [search_inds[s] for s in y_pred.argsort()[::-1][:n]]\n",
    "    elif acq == 'mli':\n",
    "        return train_inds + [search_inds[s] for s in (np.divide(y_pred-np.max(y[train_inds]),y_std)).argsort()[::-1][:n]]\n",
    "    elif acq == 'mu':\n",
    "        return train_inds + [search_inds[s] for s in y_std.argsort()[::-1][:n]]\n",
    "    else:\n",
    "        return train_inds + [s for s in np.random.choice(search_inds, n)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf381529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060a6ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# There are some off-diagonals with a correlation of 1, so drop 1 of each of these pairs\n",
    "X_df.drop(columns = ['MagpieData range NfUnfilled', 'MagpieData minimum NsUnfilled'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ff1f15",
   "metadata": {},
   "source": [
    "## Skewness\n",
    "Here, I check for skewness of the features and target. Some of the features are highly skewed due to low variance (i.e., most of the samples take on one value) - I chose to remove these from the data set. The target is also skewed and becomes more normal after log-transforming - I compare model training/prediction on both the gap and log-transformed gap in the ML section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4195c96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skew_val = X_df.skew(axis = 0)\n",
    "skewed = skew_val[ abs( skew_val ) > 1.0 ]\n",
    "skewed.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e934fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use threshold of 7.0 to identify highly skewed variables and plot their distributions\n",
    "to_plot = skewed[ abs(skewed.sort_values()) > 7.0 ].index\n",
    "boxplot(X_df, to_plot, nrows = 2)\n",
    "distplot(X_df, to_plot, nrows = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb8ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop features with low variance\n",
    "for feature in to_plot:\n",
    "    this_var = df[feature].var()\n",
    "    print('{}: {}'.format(feature, this_var))\n",
    "    if this_var < 0.1:\n",
    "        X_df.drop(columns = [feature], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1129f6",
   "metadata": {},
   "source": [
    "### Log transformations\n",
    "Does not help features, but helps target, so I only include the log-transformed target in the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b5d6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df['log_MagpieData mode NfValence'] = np.log(X_df['MagpieData mode NfValence']+1)\n",
    "boxplot(X_df, ['log_MagpieData mode NfValence'], nrows = 1, figsize = (5,5))\n",
    "distplot(X_df, ['log_MagpieData mode NfValence'], nrows = 1, figsize = (5,5))\n",
    "X_df.drop(columns = ['MagpieData mode NfValence','log_MagpieData mode NfValence'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a169ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_df['log_MagpieData mode NdUnfilled'] = np.log(X_df['MagpieData mode NdUnfilled']+1)\n",
    "boxplot(X_df, ['log_MagpieData mode NdUnfilled'], nrows = 1, figsize = (5,5))\n",
    "distplot(X_df, ['log_MagpieData mode NdUnfilled'], nrows = 1, figsize = (5,5))\n",
    "X_df.drop(columns = ['MagpieData mode NdUnfilled','log_MagpieData mode NdUnfilled'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77477f2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check target\n",
    "y.skew(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e160c09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "boxplot(df, ['gap expt'], nrows = 1, figsize = (5,5))\n",
    "distplot(df, ['gap expt'], nrows = 1, figsize = (5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1239260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_gap expt'] = np.log(y+1)\n",
    "boxplot(df, ['log_gap expt'], nrows = 1, figsize = (5,5))\n",
    "distplot(df, ['log_gap expt'], nrows = 1, figsize = (5,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe3619",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "Here, I perform PCA to see if the data is clustered. As it is not, no further clustering analysis is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efb581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data (necessary for PCA!)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame( scaler.fit_transform(X_df), columns = X_df.columns )\n",
    "# Start with 3 components\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_scaled)\n",
    "print(pca.explained_variance_ratio_)\n",
    "X_pca = pca.transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fae3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(10, 5))\n",
    "ax = fig.add_subplot(111, projection=\"3d\", elev=20, azim=134)\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=y, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now check in two dimensions\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "print(pca.explained_variance_ratio_)\n",
    "X_pca = pca.transform(X_scaled)\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2320ab6b",
   "metadata": {},
   "source": [
    "# Full model training & Prediction\n",
    "Here, I perform model training and prediction on the entire data set and evaluate the performance of each model with 5-fold cross-validation. I test the effect of log-transforming the gap, as well as removing outliers in the gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e1834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First tune without log transformation of target\n",
    "params, model, top_feat = tune(X_scaled, y, outf = 'full_model')\n",
    "cross_val_score(model, X_scaled[top_feat], y, scoring = 'neg_mean_absolute_error', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977e635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Next tune with log transformation of target\n",
    "params_log, model_log, top_feat_log = tune(X_scaled, np.log(y), outf = 'full_model_log')\n",
    "cross_val_score(model, X_scaled[top_feat_log], np.log(y), scoring = 'neg_mean_absolute_error', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926eb613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next tune remove outliers in target\n",
    "outliers_inds = np.abs(stats.zscore(y)) < 3\n",
    "y_no_out = y[outliers_inds]\n",
    "X_no_out = X_scaled.drop(outliers_inds[(outliers_inds == False)].index)\n",
    "params_no_out, model_no_out, top_feat_no_out = tune(X_no_out, np.log(y_no_out), outf = 'full_model_log_noout')\n",
    "cross_val_score(model, X_no_out[top_feat_no_out], np.log(y_no_out), scoring = 'neg_mean_absolute_error', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with no log and with outliers\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df[top_feat], y, random_state = 0)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with log and with outliers\n",
    "X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_df[top_feat_log], np.log(y), random_state = 0)\n",
    "model_log.fit(X_train_log,y_train_log)\n",
    "y_pred_log = model_log.predict(X_test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f47e7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with log and no outliers\n",
    "X_train_no_out, X_test_no_out, y_train_no_out, y_test_no_out = train_test_split(X_no_out[top_feat_no_out], \n",
    "                                                                                np.log(y_no_out), random_state = 0)\n",
    "model_no_out.fit(X_train_no_out,y_train_no_out)\n",
    "y_pred_no_out = model_no_out.predict(X_test_no_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f1de1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predvtrue(y_test.to_numpy(), y_pred)\n",
    "print(r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cdc26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predvtrue(np.exp(y_test_log.to_numpy()), np.exp(y_pred_log))\n",
    "print(r2_score(y_test_log, y_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24634822",
   "metadata": {},
   "outputs": [],
   "source": [
    "predvtrue(np.exp(y_test_no_out.to_numpy()), np.exp(y_pred_no_out))\n",
    "print(r2_score(y_test_no_out, y_pred_no_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6a29ae",
   "metadata": {},
   "source": [
    "### Observations\n",
    "All models have similar $R^2$ but vary slightly in $MAE$. Further improvements could be made with more extensive feature engineering, hyperparameter tuning, and aggregation of predictions from different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abbdc88",
   "metadata": {},
   "source": [
    "# Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309cac04",
   "metadata": {},
   "source": [
    "The goal of this section is to guide the exploration of design space to identify compositions with a high band gap. In a real life scenario, we could use active learning to choose which experiments or simulations to prioritize. \n",
    "\n",
    "We start initially with 10 data points, then incrementally add 5 at each active learning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4110d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select initial training set\n",
    "np.random.seed(100)\n",
    "\n",
    "in_train = np.zeros(len(X_scaled), dtype=bool)\n",
    "in_train[np.random.choice(len(X_scaled), 10, replace=False)] = True\n",
    "assert not np.isclose(max(y), max(y[in_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c4083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial hyperparameter tuning and feature selection. Here, I only do this once, but ideally would do at \n",
    "# every step or every number of steps. Performing feature selection is important here for avoiding the curse\n",
    "# of dimensionality (having many more features than samples).\n",
    "all_inds = set(range(len(y)))\n",
    "train = [np.where(in_train)[0].tolist()]\n",
    "train_inds = train[-1].copy()    # Initial Set\n",
    "search_inds = list(all_inds.difference(train_inds)) # All samples not in the current set\n",
    "# Hyperparameter tuning and feature selection\n",
    "params, model, top_feat = tune(X_scaled, y, np.array(train_inds), outf = 'active_model')\n",
    "X_top = np.array(X_scaled[top_feat])\n",
    "# Model training and prediction with tuned hyperparameters and features\n",
    "model = RandomForestRegressor(num_trees = params['n_estimators'], max_depth = params['max_depth'])\n",
    "model.fit(X_top[train_inds], y[train_inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1edbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Active learning steps\n",
    "n_steps = 50\n",
    "mae = np.zeros((4,n_steps))\n",
    "y_max = np.zeros((4,n_steps))\n",
    "for a,acq in enumerate(['random', 'mei', 'mli', 'mu']):\n",
    "    train = [np.where(in_train)[0].tolist()]\n",
    "    for i in range(n_steps):\n",
    "        train_inds = train[-1].copy()    # Initial Set\n",
    "        search_inds = list(all_inds.difference(train_inds)) # All samples not in the current set\n",
    "        model.fit(X_top[train_inds], y[train_inds])\n",
    "        y_pred, y_std = model.predict(X_top[search_inds], return_std = True) # Predictions\n",
    "        mae[a,i] = mean_absolute_error(y[search_inds], y_pred)\n",
    "        y_max[a,i] = max( y[train_inds] )\n",
    "        # Update training set\n",
    "        train_inds = update(X_scaled, y_pred, y_std, train_inds, acq=acq)\n",
    "        train.append(train_inds) # Storage of the current set per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa15b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare ability of each acquisition function to find materials with high gaps\n",
    "true_max = max(y)\n",
    "random_max = y_max[0,:]\n",
    "mei_max = y_max[1,:]\n",
    "mli_max = y_max[2,:]\n",
    "mu_max = y_max[3,:]\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(range(n_steps), [true_max]*n_steps, 'k', linestyle = 'dashed')\n",
    "ax.plot(range(n_steps), random_max, label='Random')\n",
    "ax.plot(range(n_steps), mei_max, label='MEI')\n",
    "ax.plot(range(n_steps), mli_max, label='MLI')\n",
    "ax.plot(range(n_steps), mu_max, label='MU')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1a8ce4",
   "metadata": {},
   "source": [
    "### Observations\n",
    "All acquisition functions to better than selecting at random and find the material with the maximum gap within 20 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83a559",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Conventional machine learning training was used to develop a model that can predict the experimental band gap purely from composition with an $R^2$ of approximately 0.7 and an $MAE$ of approximately 0.15 eV. Active learning was successful in accelerating the exploration of design space to maximize the band gap. Further development could include further feature engineering and testing of different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de810d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
